---
title: "Course diary"
author: "Petteri MÃ¤ntymaa"
date: "May 5, 2019"
output:
    pdf_document:
        latex_engine: xelatex

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse);library(ggplot2)
```

#### Week 1: The derivative of a function

Let $f: A \rightarrow \mathbb{R}$ and $x_0 \in A$. Let's assume that there is some $r>0$ for which $(x_0 - r, x_0 + r) \subset A$. Function $f$ is differentiable at $x_0$ if there exists a limit

$$
\lim_{x\rightarrow x_0} \dfrac{f(x)-f(x_0)}{x-x_0}
$$

This limit is the derivative of $f$ at $x_0$ and formula is also called the *difference quotient*.

Let's denote $x-x_0 = h$, thus $x = x_0 + h$. Then naturally as $x$ approaches $x_0$, $x-x_0 = h$ approaches $0$ so we can rewrite the difference quotient as

$$
\lim_{h\rightarrow 0} \dfrac{f(x+h)-f(x)}{h}\\
$$

As we will be dealing a lot with logarithms on the course, let's find out the derivative of the natural logarithm $log_e(x)$, but first a few words on the notation.

As we'll be handling natural or base $e$ logarithms only, I'll refer to $log(x)$ as the natural logarithm of $x$. Also, depending on application the derivative of some function $f(x)$ can be expressed in many ways $f'(x)=\frac{d}{dx}f(x)=\frac{df(x)}{dx}$. 

Let us use the latter and write the derivative of log of x as $\frac{dlog(x)}{dx}$ and use the difference quotient to find the derivative.

$$
\begin{aligned}
\frac{dlog(x)}{dx}&=\lim_{h\rightarrow 0} \dfrac{\log(x+h)-\log(x)}{h}\\
&=\lim_{h\rightarrow 0} \dfrac{\log\left(\frac{x+h}{x}\right)}{h}\\
&=\lim_{h\rightarrow 0} \dfrac{\log\left(\frac{x}{x}+\frac{h}{x}\right)}{h}\\
&=\lim_{h\rightarrow 0} \dfrac{\log\left(1+\frac{h}{x}\right)}{h}\\
&=\lim_{h\rightarrow 0} \frac{1}{h}\log\left(1+\frac{h}{x}\right)\\
&=\lim_{h\rightarrow 0}\log\left(1+\frac{h}{x}\right)^{\frac{1}{h}}\\
\end{aligned}
$$

Let $u=\frac{h}{x}$

$$
\begin{aligned}
&=\lim_{u\rightarrow 0}\log(1+u)^{\frac{1}{ux}}\\
&=\lim_{u\rightarrow 0}\frac{1}{x}\log(1+u)^{\frac{1}{u}}\\
&=\frac{1}{x}\lim_{u\rightarrow 0}\log(1+u)^{\frac{1}{u}}\\
&=\frac{1}{x}\log(e)\\
&=\frac{1}{x}\cdot 1\\
&=\frac{1}{x}\\
\end{aligned}
$$

#### Week 2: Integration by parts

In calculus, integration by parts is based upon the product rule which states that for differentiable functions $f(x)$ and $g(x)$

$$
\frac{d}{dx}(f(x)g(x))=\frac{d}{dx}f(x)g(x)+f(x)\frac{d}{dx}g(x)
$$

Or by subtracting $\frac{d}{dx}f(x)g(x)$ from both sides

$$
f(x)\frac{d}{dx}g(x)=\frac{d}{dx}(f(x)g(x))-\frac{d}{dx}f(x)g(x)
$$

If $f(x)$ and $g(x)$ are continuous they are also integrable, so by integrating both sides

$$
\int_a^b \big(f(x)\frac{d}{dx}g(x)\big)dx=\int_a^b\big(\frac{d}{dx}(f(x)g(x))\big)dx-\int_a^b\big(\frac{d}{dx}f(x)g(x)\big)dx
$$

As $fg$ is differentiable it is also continuous, so $f(x)g(x)$ is the integral function of $\frac{d}{dx}(f(x)g(x))$, thus by the fundamental theorem of calculus

$$
\int_a^b\big(\frac{d}{dx}(f(x)g(x))\big)dx= \Bigg|_a^bf(x)g(x)
$$

This gives us the integration by parts "rule"

$$
\int_a^b \big(f(x)\frac{d}{dx}g(x)\big)dx=\Bigg|_a^bf(x)g(x)-\int_a^b\big(\frac{d}{dx}f(x)g(x)\big)dx
$$
or

$$
\int f(x)g'(x)dx=f(x)g(x)-\int f'(x)g(x)dx
$$

Example:

Define $\int x\cos(x)dx$.

Let $f(x)=x$, $f'(x)=1$, $g'(x)=\cos(x)$ and $g(x)=\sin(x)$ and substitute

$$
\begin{aligned}
\int f(x)g'(x)dx&=f(x)g(x)-\int f'(x)g(x)dx\\
\int x\cos(x)dx&=x\sin(x)-\int 1\cdot\sin(x)dx\\
&=x\sin(x)-\int \sin(x)dx\\
&=x\sin(x)- (-\cos(x))+C\\
&=x\sin(x)+\cos(x)+C
\end{aligned}
$$

#### Week3: Differentiating under the integral siqn

A general example of the problem of the interchangablility can be expressed as

$$
\frac{d}{d\theta}\int_{a(\theta)}^{b(\theta)}f(x,\theta)dx
$$

where $a(\theta)>-\infty$ and $b(\theta)<\infty$.

Utilizing the Fundamental Theorem of Calculus and the chain rule, a rule differentiating under the integral siqn is called the Leibniz's Rule.

If $f(x,\theta)$, $a(\theta)$ and $b(\theta)$ are differentiable in regard to $\theta$

$$
\frac{d}{d\theta}\int_{a(\theta)}^{b(\theta)}f(x,\theta)dx=f(b(\theta),\theta)\frac{d}{d\theta}b(\theta)-f(a(\theta),\theta)\frac{d}{d\theta}a(\theta)+\int_{a(\theta)}^{b(\theta)}\frac{\partial}{\partial\theta}f(x,\theta)dx
$$

And if $a(\theta)$ and $b(\theta)$ are constants then, as the derivatives of the constants are 0, the Leibniz rule can be simplified as

$$
\frac{d}{d\theta}\int_{a}^{b}f(x,\theta)dx = \int_{a}^{b}\frac{\partial}{\partial\theta}f(x,\theta)dx
$$

#### Week 4: Interchangability of limit and integration (continuing from previous week)

As discussed earlier the derivative is a limit so the central issue is the interchangability of the limits and integration

$$
\frac{\partial}{\partial\theta}f(x,\theta)=\lim_{h\rightarrow0} \frac{f(x,\theta)+h-f(x,\theta)}{h}
$$

so

$$
\int_{-\infty}^{\infty}\frac{\partial}{\partial\theta}f(x,\theta)=\int_{-\infty}^{\infty}\lim_{h\rightarrow0} \frac{f(x,\theta+h)-f(x,\theta)}{h}
$$
and
$$
\frac{d}{d\theta}\int_{-\infty}^{\infty}f(x,\theta)=\lim_{h\rightarrow0}\int_{-\infty}^{\infty}\frac{f(x,\theta+h)-f(x,\theta)}{h}
$$

The generalisation of this problem is covered in measure theory, more specifically Lebesque's Dominated Convergence Theorem, which was not very helpful in increasing my understanding of the issue. What I was able to gather, and I hope I'm not too far off, if the integral is fairly well-behaved, for example the integral being finite and the function to be integraded continuous, the limits and the integral are interchangable.

#### Week 5: A Sufficient Statistic

Say we are interested in making inferences about an unknown parameter $\theta$ from information provided by sample of random variables $X_1,\dots,X_n$ or $\mathbf{X}$. A usual prospect is to try to summarize the information in the sample regarding the parameter of interest $\theta$ by some function of the sample $T(\mathbf{T})$, probably the most common ones being sample means, sample variances, sample totals.

*The sufficiency principle* outlines a way of obtaining a statistic that preserves as much of the information about $\theta$ as possible. Thus, a *sufficient statistic* for $\theta$ is a statistic that holds all the information about $\theta$ obtained from the sample.

That is, if we have two realized sample values $\mathbf{x}$ and $\mathbf{y}$ with $T(\mathbf{x})=T(\mathbf{y})$ any conclusions about $\theta$ should not differ whether we observe $\mathbf{X}=\mathbf{x}$ or $\mathbf{Y}=\mathbf{y}$.

In other words (for an example in a discrete case) it must be shown that $P_\theta(\mathbf{X}=\mathbf{x})=P_\theta(\mathbf{Y}=\mathbf{y}),\quad \forall x,\theta$ 

$$
\begin{aligned}
P_\theta(\mathbf{X}=\mathbf{x}|T(\mathbf{X})=T(\mathbf{x})) &= \frac{P_\theta(\mathbf{X}=\mathbf{x}\quad\cap\quad T(\mathbf{X})=T(\mathbf{x}))}{P_\theta(T(\mathbf{X})=T(\mathbf{x}))}\\
&= \frac{P_\theta(\mathbf{X}=\mathbf{x})}{P_\theta(T(\mathbf{X})=T(\mathbf{x}))}\\
&= \frac{p(\mathbf{x}|\theta)}{q(T(\mathbf{x})|\theta)}\\
\end{aligned}
$$

where $p(\mathbf{x}|\theta)$ and $q(T(\mathbf{x})|\theta)$ are the probability mass functions for the sample and for $T(\mathbf{X})$ accordingly. Finally, $T(\mathbf{X})$ is a sufficient statistic *iff* for every $\mathbf{x}$ the ratio $\frac{p(\mathbf{x}|\theta)}{q(T(\mathbf{x})|\theta)}$ is constant as a function of $\theta$.

#### Week 6: Maximum likelihood estimator

To sum up the previous week, any statistic, as in a function of a sample, is a point *estimator* and a point *estimate* is a realized value of an estimator. That is, an estimator $T(\mathbf{X})$ is a function of random variables $X_1,\dots,X_n$ and an estimate $T(\mathbf{x})$ is a function of the realized values $x_1,\dots,x_n$.

Given an independent and identically distributed sample $X_1,\dots,X_n$ with a pmf/pdf $f(x|\theta_1,\dots,\theta_k)$ the likelihood function is

$$
L(\theta|\mathbf{x})=L(\theta_1,\dots,\theta_k|x_1,\dots,x_k)=\prod_{i=1}^{n}f(x|\theta_1,\dots,\theta_k)
$$

The prospect is finding, for each sample point $\mathbf{x}$, a parameter value $\hat{\theta}(\mathbf{x})$ which *maximizes* $L(\theta|\mathbf{x})$ as a function of $\theta$. For sample of random variables $\mathbf{X}$ an maximum likelihood estimator is $\hat{\theta}(\mathbf{X})$.

Maximizing denotes the process of finding the global maximum, so two steps are in order. Recalling that the points where a derivative of a function is 0 are inflection points, local minima or maxima and global minima or maxima, so as a first step, if $L$ is differentiable, solving on which $\theta_i,\dots,\theta_k$
$$
\frac{\partial}{\partial\theta_i}L(\theta|\mathbf{x})=0
$$
narrows the search to the possible values for the maximum likelihood estimate or MLE.

Also be noted that in many cases $\log(L(\theta|\mathbf{X}))=l(\theta|\mathbf{x})$ can be more straightforward to differentiate as long as the log is also concave. This is the case at least with most probability distributions, especially in the exponential family.

Once a solution to the first derivative is found, it must be shown that the value is at least a local maximum. This is done by finding out if a second derivative exist and that it is negative

$$
\frac{d^2}{d\theta^2}L(\theta|\mathbf{x})|_{\theta=\hat{\theta}}<0
$$

Example:

Let $X_1,\dots,X_n\sim Exp(\lambda)$ and $\lambda>0$

$$
\begin{aligned}
f(\mathbf{x}|\lambda) &= \lambda \exp(-\lambda \mathbf x)\\
L(\lambda|\mathbf{x})&=\prod_{i=1}^nf(x_i|\lambda)=\lambda^n \exp(-\lambda(x_1+\dots+x_n)) 
\end{aligned}
$$
For convenience denote $\bar{x}=(x_1+\dots+x_n)/n$

$$
L(\lambda|\mathbf{x})=\prod_{i=1}^nf(x_i|\lambda)=\lambda^n \exp(-\lambda n\bar{x})
$$

As $E(X_i)=\frac{1}{\lambda}$ reparametrize $\mu=1/\lambda$

$$
\begin{aligned}
L(\mu|\mathbf{x})&=\mu^{-n} \exp(\frac{-n\bar{x}}{\mu})\\
\end{aligned}
$$

Then take the logarithm

$$
\begin{aligned}
\log(L(\mu|\mathbf{x}))=l(\mu|\mathbf{x}) &= \log(\mu^{-n} \exp(\frac{-n\bar{x}}{\mu}))\\
&= \log(\mu^{-n})+ \log(\exp(\frac{-n\bar{x}}{\mu}))\\
&= -n\log(\mu) + \frac{-n\bar{x}}{\mu}\\
\end{aligned}
$$

and the first derivative

$$
\begin{aligned}
\frac{\partial}{\partial\mu}l(\mu|\mathbf{x}) &= \frac{d}{d\mu} \Big(-n\log(\mu) + \frac{-n\bar{x}}{\mu}\Big)\\
&= \frac{d}{d\mu} \Big(-n\log(\mu)\Big) + \frac{d}{d\mu}\Big(\frac{-n\bar{x}}{\mu}\Big)\\
&= -n\frac{d}{d\mu}\Big(\log(\mu)\Big)-n\bar{x}\frac{d}{d\mu}\Big(\frac{1}{\mu}\Big)\\
&= \frac{-n}{\mu}+\frac{n\bar{x}}{\mu^2}\\
\end{aligned}
$$

Then solve for $\frac{\partial}{\partial\theta_i}l(\theta|\mathbf{x})=0$

$$
\begin{aligned}
\frac{-n}{\mu}+\frac{n\bar{x}}{\mu^2} &= 0\\
\frac{n\bar{x}}{\mu^2} &= \frac{n}{\mu}\\
\frac{n\bar{x}}{\mu} &= n\\
\frac{n}{\mu} &= \frac{n}{\bar{x}}\\
\hat{\mu} &= \bar{x}=\frac{1}{\hat{\lambda}}
\end{aligned}
$$

Lastly will verify $\frac{d^2}{d\theta^2}L(\theta|\mathbf{x})|_{\theta=\hat{\theta}}<0$

$$
\begin{aligned}
\frac{\partial^2}{\partial\mu^2}l(\mu|\mathbf{x}) &= \frac{d}{d\mu}\Big(\frac{-n}{\mu}+\frac{n\bar{x}}{\mu^2}\Big)\\
 &= -n\frac{d}{d\mu}\Big(\frac{1}{\mu}\Big) + n\bar{x}\frac{d}{d\mu}\Big(\frac{1}{\mu^2}\Big)\\
 &= \frac{n}{\mu^2} - \frac{2n\bar{x}}{\mu^3}\\
\end{aligned}
$$

Simplify by putting fractions over a common denominator

$$
\begin{aligned}
\frac{n}{\mu^2} - \frac{2n\bar{x}}{\mu^3} &= \frac{\mu n}{\mu^3}-\frac{2n\bar{x}}{\mu^3}\\
&=\frac{n(\mu-2\bar{x})}{\mu^3}
\end{aligned}
$$

We know that $\hat{\mu}=\bar{x}$, thus

$$
\begin{aligned}
\frac{n(\hat{\mu}-2\hat{\mu})}{\mu^3} &< 0\\
\frac{n(-\hat{\mu})}{\mu^3} &< 0\\
-\frac{n}{\hat{\mu^2}} &< 0
\end{aligned}
$$

Which is true for $\mu>0$.
